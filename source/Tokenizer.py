# ordinal handling
from number_parser import parse
from num2words import num2words
# Finds dates of the form YYYY-MM-DDDD currently not working
import calendar
# handles parsing of brackets
import re
# TokenSet and Tokens to be used as output for the Tokenizer
from dataclasses import dataclass, field
from typing import Set

from source.pyLookupParser.Tokenizer import *
from source.pyLookupParser.Signature import *

@dataclass
class Token:
    """
    One of the outputs of the Tokenizer. Is initialized with the three attributes.
    Post_init later defines a forth attribute called 'token' which is a combination in form of a tuple.
    """
    keyword: str
    category: str
    weight: float

    def __post_init__(self):
        self.token = (self.keyword, self.category, self.weight)


@dataclass
class TokenSet:
    """
    The complete output that is generated by the Tokenizer.
    Initialization is done with no input.
    It can be extended by adding '+' tokens of class Token to it.
    It can further be used as iteration.
    """
    tokens: Set[tuple] = field(default_factory=set)

    def __add__(self, others):
        assert type(others) == Token or TokenSet, "The datatype of other is not chosen correctly."
        if type(others) == Token:
            self.tokens.add(others.token)
        elif type(others) == TokenSet:
            self.tokens.union(others.tokens)
        return TokenSet(tokens=self.tokens)
    
    def __iter__(self):
        return iter(self.tokens)
    
    def __len__(self):
        return len(self.tokens)
    
    def contains(self, other):
        assert type(other) == tuple
        return other in self.tokens
        
    def equals(self, other):
        assert type(other) == TokenSet
        result = True
        for token in other.tokens:
            if not self.contains(token):
                result = False
        for token in self.tokens:
            if not other.contains(token):
                result = False
        return result

class Tokenizer:
    """
    Static class containing functions to obtain equivalent expressions.
    Gives the results as keys in a TokenSet, 
    that also contains a metric for the usefulness of the synonym in the value of the entry based
    on Semantified CEUR-WS and SemPubFlow: a
    Metadata-First scientific publishing workflow
    leveraging LLMs and Wikidata
    Wolfgang Fahl et al. (f1-score times 10, rounded to the nearest integer).
    
    Currently:
    Without superfluous spaces, ordinals to mathform, ordinals to textform
    Uses abbreviation (assumed to be the only expression in brackets or all uppercase)
    Gives out the date of the paper
    Gives out infixes of the title (low scores)
    """
    # needed for infixes
    REDUNDANT_STRINGS = ['ieee', 'iop', 'ieee/acm', 'edp', 'elsevier', 
                         'spie', 'annual', 'yearly', 'meeting', 'symposium', 
                         'workshop', 'conference', 'conferences', 'proceeding', 
                         'proceedings', 'or', 'and', '&', 'of', 'on', 
                         'for', 'at', 'about']
    # Months
    MONTHS = dict((month, index) 
                  for index, month in enumerate(calendar.month_abbr) if month)
    
    def __init__(self):
        """
        Initialize empty tokenset.
        """
        self.tokenset = TokenSet()
   
    # Helpermethods
    @staticmethod
    def _is_decimal(string: str) -> bool:
        """
        Checks if a string represents a decimal
        """
        try:
            int(string)
            return True
        except ValueError:
            return False

    @staticmethod
    def _number_to_ordinal_number(n: int) -> str:
        """
        Transforms a number (int) to an ordinal number. 
        No testing of it is in the correct range
        """
        if 11 <= (n % 100) <= 13:
            suffix = 'th'
        else:
            suffix = ['th', 'st', 'nd', 'rd', 'th'][min(n % 10, 4)]
        return str(n) + suffix
    
    @staticmethod
    def _ordinal_to_text_number(ordinal: tuple) -> str:
        """
        Transforms an ordinal number (like \"6th\")
        to its text pendant (\"sixth\").
        Input: Found string, and suffix of string (output of regex method).
        """
        num = int(ordinal[0][:-len(ordinal[1])])
        text_ordinal =  num2words(num, ordinal=True)
        return text_ordinal
        

    #Main Methods

    def analyzeTitle(self, input_string: str) -> None: 
        """
        Test all possible variations of a string 
        based on certain conditions (explained in the method).

        Input: String to generate synonymes with
        Output: Dictionary: Keys:Synonymes Values:Scores of the synonymes (How important)
        """
       
        # Plane Title
        string = re.sub(r'\s{2,}', ' ', input_string)
        string = string.rstrip()  # get rid of multiple whitespaces
        self.tokenset += Token(string, "Full Title", 1)
        
        # Title change in text format (Convert ordinals to text)
        re_results = re.findall(r'(\d+(st|nd|rd|th))', string, flags=re.IGNORECASE)
        altered_string = string
        for res, suffix in re_results:
            num_text = Tokenizer._ordinal_to_text_number((res, suffix))
            altered_string = altered_string.replace(res, num_text)

        if altered_string != string:
            self.tokenset += Token(altered_string, "Modified Title", 1)
        else:
            pass
          
        # Title change in numerical format (Convert text ordinals to numbers)
        words = string.split()
        for i, word in enumerate(words):
            if len(word) > 2 and (word[:-2] == r"(?i)nd" or word[:-2] == r"(?i)st" or word[:-2] == r"(?i)th" or word[:-2] == r"(?i)rd"):
                # Text to number
                parsed_word = parse(word[:-2])
                text_ordinal = parsed_word
                
                if parsed_word != word:  # only if parsing was successful change to ordinal
                    new_number = Tokenizer._number_to_ordinal_number(int(parsed_word))
                    words[i] = new_number
                else:
                    pass
            else:
                pass

        numerical_form = (" ").join(words)
        numerical_form = re.sub(r'\s{2,}', ' ', numerical_form)

        changed_in_numerical_format = False  
        if numerical_form != string:
            changed_in_numerical_format = True
            self.tokenset += Token(numerical_form, "Modified Title", 1)
        else:
            pass

        # Ordinals (only if there is no ambiguity)
        if len(re_results) == 1 and not changed_in_numerical_format:
            self.tokenset += Token(res, "Ordinal", 0.4)
            self.tokenset += Token(num_text, "Ordinal", 0.4)
        elif len(re_results) == 0 and changed_in_numerical_format:
            self.tokenset += Token(new_number, "Ordinal", 0.4)
            self.tokenset += Token(text_ordinal, "Ordinal", 0.4)
        else:
            pass

        # Abbreviation in Title using PyLookup
        pylookup_tokenizer_acr = TokenizerParser([AcronymCategory()])
        item = {"title": string}
        token = pylookup_tokenizer_acr.tokenize(string, item)

        for t in token.getTokenOfCategory("acronym"):
           self.tokenset += Token(t.tokenStr, "Acronym", 0.43)
       
      
        # Infixes
        infix_list = []
        point_infixes = string.split(". ")  # separation with ". "
        if len(point_infixes) > 1:
            for point_part in point_infixes:
                self.tokenset += Token(point_part, "Infix", 0.15)
                comma_infixes = point_part.split(", ")  # separation with ", "
                if len(comma_infixes) > 1:
                    for comma_part in comma_infixes:
                        self.tokenset += Token(comma_part, "Infix", 0.15)
                        infix_list.append(comma_part)
                else:
                    infix_list.append(point_part)
        else:
            comma_infixes = string.split(", ")
            if len(comma_infixes) > 1:
                for comma_part in comma_infixes:
                    self.tokenset += Token(comma_part, "Infix", 0.15)
                    infix_list.append(comma_part)
            else:
                pass

        if len(infix_list) > 0:
            for infix in infix_list:
                for substring in Tokenizer.REDUNDANT_STRINGS:
                    if substring in infix.lower():
                        new_infix = infix.lower().replace(" "+substring+" ", " ")
                        if new_infix != string.lower():
                            new_infix = re.sub(r'\s{2,}', ' ', new_infix).rstrip()
                            self.tokenset += Token(new_infix, "Infix", 0.15)
                        else:
                            pass
                    else:
                        pass
        else:
            for substring in Tokenizer.REDUNDANT_STRINGS:
                if substring in string.lower():
                    new_infix = string.lower().replace(" "+substring+" ", " ")
                    if new_infix != string.lower():
                        new_infix = re.sub(r'\s{2,}', ' ', new_infix).rstrip()
                        self.tokenset += Token(new_infix, "Infix", 0.15)
                    else:
                        pass
                else:
                    pass

        # Part of parantheses
        parant_found = re.findall(r'\((.*?)\)', string)
        if len(parant_found) > 0:
            for match in parant_found:
                self.tokenset += Token(match, "In paranthesis", 0.2)

        
    
    def analyzeDesciption(self, input_string: str) -> None:
        """
        (Not implemented)
        Checks the description in proceedings.com for country, city (not implemented) and date (not implemented) using pyLookupParser.
        Input: string representing the description
        """

        # prepare Description
        string = input_string
        for char in [",", "."]:
            string = string.replace(char, "")


        # Country
        pylookup_tokenizer_country = TokenizerParser([CountryCategory()])
        item = {"title": string}
        token = pylookup_tokenizer_country.tokenize(string, item)

        for t in token.getTokenOfCategory("country"):
            self.tokenset += Token(t.value, "Country Identifier", 0.5)
            self.tokenset += Token(t.tokenStr, "Country", 0.5)
           
        # City
        pylookup_tokenizer_city = TokenizerParser([CityCategory()])
        item = {"title": string}
        token = pylookup_tokenizer_city.tokenize(string,item)

        for t in token.getTokenOfCategory("city"):
            self.tokenset += Token(t.value, "City Identifier", 0.75)
            self.tokenset += Token(t.tokenStr, "City", 0.75)

        #Year
        pylookup_tokenizer_year = TokenizerParser([YearCategory()])
        item = {"title": string}
        token = pylookup_tokenizer_year.tokenize(string,item)

        for t in token.getTokenOfCategory("year"):
            self.tokenset += Token(t.tokenStr, "Year", 0.82)
    

    def check_semantics(self) -> None:
        """
        Check if the token is feasible for the searchengine.
        If the number of parantheses part are unequal, 
        we delete all parantheses of the keyword. 
        """
        ts_new = TokenSet()
        for token in self.tokenset:
            if token[0].count("(") != token[0].count(")"):
                new_token = Token(token[0].replace("(", "").replace(")", ""), 
                                  token[1], 
                                  token[2])
                ts_new += new_token
            else:
                ts_new += Token(token[0], token[1], token[2])
        self.tokenset = ts_new

    # main

    def tokenizeProceedings(self, input_dict: dict) -> TokenSet:
        """
        Creates token from the index-th entry in the given pandas dataframe, 
        structured as the proceedings.com excel sheet.
        """

        Tokenizer.analyzeTitle(self, input_string = input_dict['Conference Title'])
          
        if 'Publisher' in input_dict:
            self.tokenset += Token(str(input_dict['Publisher']),"Publisher", 0.1)        
        if 'Description' in input_dict:
            Tokenizer.analyzeDesciption(self, input_string = input_dict['Description'])

        # add short_name with year if year and short_name is found:
        found_acronym = False
        found_year = False
        for tok in self.tokenset:
            if tok[1] == 'Acronym':
                found_acronym = True
                acronym = tok[0]
            elif tok[1] == "Year":
                found_year = True
                year = tok[0]
            else:
                pass
        if found_acronym and found_year:
            self.tokenset += Token(acronym + " " + year, "Acronym with Year", 0.8)
        else:
            pass
        
        # checks if semantics with parantheses are correct
        Tokenizer.check_semantics(self)

        return self.tokenset


if __name__ == "__main__":
     dictionary = {"Conference Title": "INTELLIGENT SYSTEMS FOR MOLECULAR BIOLOGY (ISMB). ANNUAL INTERNATIONAL CONFERENCE. 14TH 2006.   ISMB 2006", 
                   "Book Title": "14th Annual International Conference on Intelligent Systems for Molecular Biology (ISMB 2006)", 
                   "Series": "", 
                   "Description": "Held 6-10 August 2006, Fortaleza, Brazil.",  # the problem is the dots and commas
                   "Mtg Year": "2006", 
                   "Publisher": "Curran Associates, Inc.", 
                   "Publ Year": "2007"}
     
     tok = Tokenizer()
     print(tok.tokenizeProceedings(input_dict=dictionary))
     