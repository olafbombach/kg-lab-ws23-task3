from dataclasses import dataclass, field
from typing import Optional, List, Set, Union
from source.Tokenizer import Tokenizer
from source.SearchEngine import SearchEngine
import polars as pl


@dataclass
class Token:
    """
    One of the outputs of the Tokenizer. Is initialized with the three attributes.
    Post_init later defines a forth attribute called 'token' which is a combination in form of a tuple.
    """
    keyword: str
    category: str
    weight: float

    def __post_init__(self):
        self.token = (self.keyword, self.category, self.weight)


@dataclass
class TokenSet:
    """
    The complete output that is generated by the Tokenizer.
    Initialization is done with no input.
    It can be extended by adding '+' tokens of class Token to it.
    """
    tokens: set = field(default_factory=set)

    def __add__(self, others):
        assert type(others) == Token, "The datatype of other is not chosen correctly."
        self.tokens.add(others.token)
        return TokenSet(tokens=self.tokens)


@dataclass
class ProceedingsEvent:
    """
    The instance class of a Proceedings.com event.
    In this all information can be stored during one process run.
    """
    input_info: dict
    keywords: TokenSet = None

    full_title: str = None
    short_name: Optional[str] = None
    ordinal: Optional[int] = None
    part_of_series: Optional[str] = None
    country_name: Optional[str] = None
    city_name: Optional[str] = None
    main_object: Optional[str] = None
    year: Optional[int] = None
    start_time = None
    end_time = None

    def apply_tokenizer(self):
        self.keywords = Tokenizer.tokenizeProceedings(self.input_info)

    def apply_searchengine(self, se: SearchEngine):
        search_hits = se.search_dict(self.keywords.tokens)
        if se.get_dataset_name == "Wikidata":
            for pos in range(search_hits.shape[0]):
                wikievent = WikidataEvent(input_info=search_hits.row(pos, named=True))

        elif se.get_dataset_name == "proceedings.com":
            pass


    '''
    def apply_semantifier(self):
        se = NLP()
        output = se.semantify(self.input_info)
        self.full_title = output['full_title']

        self.full_title: str = None
        self.short_name: Optional[str] = None
        ordinal: Optional[int] = None
        part_of_series: Optional[str] = None
        country_name: Optional[str] = None
        city_name: Optional[str] = None
        main_object: Optional[str] = None
        year: Optional[int] = None
        start_time = None
        end_time = None

    def apply_encoder(self):
    '''


@dataclass
class WikidataEvent:
    """
    The instance class of a Wikidata event. In this all information can be stored ruing one process run. (tbc)
    """
    input_info: dict
    keywords_score: Optional[float] = None

    # after semantification
    full_title: str = None
    short_name: Optional[str] = None
    ordinal: Optional[int] = None
    part_of_series: Optional[str] = None
    country_name: Optional[str] = None
    city_name: Optional[str] = None
    main_object: Optional[str] = None
    year: Optional[int] = None
    start_time = None
    end_time = None

    '''def apply_searchengine(self, source: str = "Wikidata", fastsearch: bool = "False"):
        se = SearchEngine(source, f_seach=fastsearch)
        pass'''


@dataclass
class EventSeries:
    """
    Not sure yet, where we would have to use this.
    """
    collection: List[Union[ProceedingsEvent, WikidataEvent]]


def main():
    ts = TokenSet()
    print(ts)
    tok1 = Token(keyword="eight", category="ordinal", weight=40)
    ts = ts + tok1
    print(ts)
    tok2 = Token(keyword="bcip", category="title", weight=80)
    ts = ts + tok2
    print(ts)
    tok3 = Token(keyword="eight", category="ordinal", weight=40)
    ts = ts + tok3
    print(ts)


if __name__ == "__main__":
    main()
